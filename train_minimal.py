import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# =======================
# Dataset –¥–ª—è –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤
# =======================
class LatentPromptDataset(Dataset):
    """Dataset –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    def __init__(self, latents_dir, prompts_dir):
        self.latents_dir = latents_dir
        self.prompts_dir = prompts_dir
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
        self.latent_files = sorted([f for f in os.listdir(latents_dir) if f.endswith('.pt')])
        self.prompt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])
        
        assert len(self.latent_files) == len(self.prompt_files), \
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–æ–≤ ({len(self.latent_files)}) != –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ ({len(self.prompt_files)})"
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.latent_files)} –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤")

    def __len__(self):
        return len(self.latent_files)

    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–∞—Ç–µ–Ω—Ç
        latent_path = os.path.join(self.latents_dir, self.latent_files[idx])
        latent = torch.load(latent_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()
        
        return latent, prompt

# =======================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Consistency Distillation
# =======================
def get_text_embeddings(prompts, model):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –º–æ–¥–µ–ª–∏
    tokenized = model.tokenizer.tokenize(prompts)
    device = next(model.parameters()).device
    input_ids = tokenized['input_ids'].to(device)
    
    with torch.no_grad():
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ text_encoder –º–æ–¥–µ–ª–∏
        text_embeddings = model.text_encoder.encode(input_ids)[0]
    
    return text_embeddings.to("cuda")  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

def consistency_distillation_step(latents, text_embeddings, teacher_model, student_model):
    """
    –û–¥–∏–Ω —à–∞–≥ Consistency Distillation
    """
    batch_size = latents.shape[0]
    
    # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞ –∏–∑ EDM —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    rnd_normal = torch.randn([batch_size, 1, 1, 1], device=latents.device)
    sigma = (rnd_normal * teacher_model.edm_config.P_std + teacher_model.edm_config.P_mean).exp()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —á–∏—Å—Ç—ã–º –ª–∞—Ç–µ–Ω—Ç–∞–º
    noise = torch.randn_like(latents) * sigma
    noisy_latents = latents + noise
    
    # Teacher: –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞ —Å —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞
    with torch.no_grad():
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É—á–∏—Ç–µ–ª—è
        teacher_latents = noisy_latents.to(next(teacher_model.parameters()).device)
        teacher_sigma = sigma.to(next(teacher_model.parameters()).device)
        teacher_embeddings = text_embeddings.to(next(teacher_model.parameters()).device)
        
        teacher_output = teacher_model.model_forward_wrapper(
            teacher_latents.float(),
            teacher_sigma,
            teacher_embeddings.float(),
            teacher_model.dit,
            mask_ratio=0.0
        )
        teacher_denoised = teacher_output['sample'].to("cuda")  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞ GPU
    
    # Student: —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    student_output = student_model.model_forward_wrapper(
        noisy_latents.float(),
        sigma,
        text_embeddings.float(),
        student_model.dit,
        mask_ratio=0.0
    )
    student_denoised = student_output['sample']
    
    # Consistency loss: —Å—Ç—É–¥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ –∂–µ, —á—Ç–æ –∏ —É—á–∏—Ç–µ–ª—å
    loss = nn.MSELoss()(student_denoised, teacher_denoised)
    
    return loss

# =======================
# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª
# =======================
def custom_collate(batch):
    """
    –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∞—Ç–µ–Ω—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ–º–ø—Ç—ã
    """
    latents_list = []
    prompts_list = []
    
    for latent, prompt in batch:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ª–∞—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å [C, H, W]
        if latent.dim() == 4 and latent.shape[0] == 1:
            latent = latent.squeeze(0)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é batch dimension
        
        latents_list.append(latent)
        prompts_list.append(prompt)
    
    # –°—Ç–∞–∫–∞–µ–º –ª–∞—Ç–µ–Ω—Ç—ã –≤ –±–∞—Ç—á [B, C, H, W]
    latents_batch = torch.stack(latents_list, dim=0)
    
    return latents_batch, prompts_list

def train_consistency_distillation(dataloader, teacher_model, student_model, optimizer, num_epochs=10):
    """
    –û–±—É—á–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Consistency Distillation
    """
    losses_history = []
    
    print(f"\n{'='*60}")
    print(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {num_epochs} —ç–ø–æ—Ö, {len(dataloader)} –±–∞—Ç—á–µ–π –Ω–∞ —ç–ø–æ—Ö—É")
    print(f"{'='*60}\n")
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        student_model.train()
        num_batches = 0
        
        for i, (latents, prompts) in enumerate(dataloader):
            try:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ª–∞—Ç–µ–Ω—Ç—ã –Ω–∞ GPU
                latents = latents.to("cuda").float()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–æ–≤
                if latents.dim() == 3:
                    # –ï—Å–ª–∏ –Ω–µ—Ç batch dim, –¥–æ–±–∞–≤–ª—è–µ–º
                    latents = latents.unsqueeze(0)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–∞—Ç–µ–Ω—Ç—ã –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                assert latents.shape[1] == 4, f"–û–∂–∏–¥–∞–µ—Ç—Å—è 4 –∫–∞–Ω–∞–ª–∞, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[1]}"
                assert latents.shape[2] == 64 and latents.shape[3] == 64, \
                    f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä 64x64, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[2]}x{latents.shape[3]}"
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                text_embeddings = get_text_embeddings(prompts, teacher_model)
                
                # Consistency distillation step
                loss = consistency_distillation_step(
                    latents, text_embeddings, teacher_model, student_model
                )
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                optimizer.zero_grad()
                loss.backward()
                
                # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                torch.nn.utils.clip_grad_norm_(student_model.dit.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                if i % 10 == 0:
                    print(f"–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}] | –ë–∞—Ç—á [{i}/{len(dataloader)}] | Loss: {loss.item():.6f}")
                    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}: {e}")
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                torch.cuda.empty_cache()
                continue
        
        # –°—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å –∑–∞ —ç–ø–æ—Ö—É
        if num_batches > 0:
            avg_epoch_loss = epoch_loss / num_batches
            losses_history.append(avg_epoch_loss)
            
            print(f"\n{'='*60}")
            print(f"–≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –°—Ä–µ–¥–Ω–∏–π Loss: {avg_epoch_loss:.6f}")
            print(f"{'='*60}\n")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 2 —ç–ø–æ—Ö–∏
            if (epoch + 1) % 2 == 0:
                checkpoint_path = f"student_checkpoint_epoch_{epoch+1}.pt"
                torch.save(student_model.dit.state_dict(), checkpoint_path)
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}\n")
        else:
            print(f"‚ö†Ô∏è –≠–ø–æ—Ö–∞ {epoch+1}: –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –±–∞—Ç—á–µ–π!")
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return losses_history

# =======================
# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
# =======================
if __name__ == "__main__":
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å GPU –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        torch.cuda.synchronize()
        print(f"üßπ GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –°–≤–æ–±–æ–¥–Ω–æ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    latents_dir = r"C:\newTry2\train\datadir\latents"
    prompts_dir = r"C:\newTry2\train\datadir\prompts"
    
    # =======================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —É—á–∏—Ç–µ–ª—è –∏ —Å—Ç—É–¥–µ–Ω—Ç–∞
    # =======================
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —É—á–∏—Ç–µ–ª—è...")
    try:
        teacher_model = create_latent_diffusion(
            latent_res=64,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –≤–µ—Å–∞–º–∏
            in_channels=4,  # VAE latents –∏–º–µ—é—Ç 4 –∫–∞–Ω–∞–ª–∞
            pos_interp_scale=2.0,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π scale
            precomputed_latents=False,  # –ú—ã –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã –≤ forward
            dtype="float16"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏...")
        teacher_model = teacher_model.to("cpu")
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–∞ CPU")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ CPU...")
        teacher_model = create_latent_diffusion(
            latent_res=64,
            in_channels=4,
            pos_interp_scale=2.0,
            precomputed_latents=False,
            dtype="float16"
        ).to("cpu")
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —É—á–∏—Ç–µ–ª—è
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —É—á–∏—Ç–µ–ª—è...")
    try:
        device = "cuda" if next(teacher_model.parameters()).is_cuda else "cpu"
        teacher_weights = torch.load("./micro_diffusion/trained_models/teacher.pt", map_location=device)
        teacher_model.dit.load_state_dict(teacher_weights, strict=False)
        teacher_model.eval()  # –£—á–∏—Ç–µ–ª—å –≤—Å–µ–≥–¥–∞ –≤ —Ä–µ–∂–∏–º–µ eval
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –Ω–∞ CPU...")
        teacher_weights = torch.load("./micro_diffusion/trained_models/teacher.pt", map_location="cpu")
        teacher_model.dit.load_state_dict(teacher_weights, strict=False)
        teacher_model.eval()
        print("‚úÖ –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ CPU")

    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞...")
    student_model = create_latent_diffusion(
        latent_res=64,
        in_channels=4,
        pos_interp_scale=2.0,
        precomputed_latents=False,
        dtype="float16"
    ).to("cuda")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—É–¥–µ–Ω—Ç–∞ –≤–µ—Å–∞–º–∏ —É—á–∏—Ç–µ–ª—è
    student_model.dit.load_state_dict(teacher_weights, strict=False)
    student_model.train()  # –°—Ç—É–¥–µ–Ω—Ç –≤ —Ä–µ–∂–∏–º–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤–µ—Å–∞–º–∏ —É—á–∏—Ç–µ–ª—è")

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞
    optimizer = optim.Adam(student_model.dit.parameters(), lr=1e-5)
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å lr={1e-5}\n")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    dataset = LatentPromptDataset(latents_dir, prompts_dir)
    dataloader = DataLoader(
        dataset, 
        batch_size=1,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ 
        shuffle=True, 
        num_workers=0,
        collate_fn=custom_collate  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞
    )
    print(f"‚úÖ DataLoader —Å–æ–∑–¥–∞–Ω: {len(dataset)} —Å—ç–º–ø–ª–æ–≤, batch_size=1\n")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    num_epochs = 10
    losses = train_consistency_distillation(
        dataloader, teacher_model, student_model, optimizer, num_epochs
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    final_model_path = "student_final.pt"
    torch.save(student_model.dit.state_dict(), final_model_path)
    print(f"\nüíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
    
    # –ì—Ä–∞—Ñ–∏–∫ –ª–æ—Å—Å–∞
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, num_epochs + 1), losses, marker='o', linewidth=2)
    plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Consistency Distillation Training Loss', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: training_loss.png")