import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è CPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# =======================
# Dataset –¥–ª—è –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤
# =======================
class LatentPromptDataset(Dataset):
    """Dataset –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    def __init__(self, latents_dir, prompts_dir):
        self.latents_dir = latents_dir
        self.prompts_dir = prompts_dir
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
        self.latent_files = sorted([f for f in os.listdir(latents_dir) if f.endswith('.pt')])
        self.prompt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])
        
        assert len(self.latent_files) == len(self.prompt_files), \
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–æ–≤ ({len(self.latent_files)}) != –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ ({len(self.prompt_files)})"
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.latent_files)} –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤")

    def __len__(self):
        return len(self.latent_files)

    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–∞—Ç–µ–Ω—Ç
        latent_path = os.path.join(self.latents_dir, self.latent_files[idx])
        latent = torch.load(latent_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()
        
        return latent, prompt

# =======================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Consistency Distillation
# =======================
def get_text_embeddings(prompts, model):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"""
    tokenized = model.tokenizer.tokenize(prompts)
    device = next(model.parameters()).device
    input_ids = tokenized['input_ids'].to(device)
    
    with torch.no_grad():
        text_embeddings = model.text_encoder.encode(input_ids)[0]
    
    return text_embeddings

def consistency_distillation_step_debug(latents, text_embeddings, teacher_model, student_model):
    """–û–¥–∏–Ω —à–∞–≥ Consistency Distillation —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
    print(f"üîç DEBUG: latents.shape = {latents.shape}, dtype = {latents.dtype}")
    print(f"üîç DEBUG: text_embeddings.shape = {text_embeddings.shape}, dtype = {text_embeddings.dtype}")
    
    batch_size = latents.shape[0]
    
    # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞ –∏–∑ EDM —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    rnd_normal = torch.randn([batch_size, 1, 1, 1], device=latents.device)
    sigma = (rnd_normal * teacher_model.edm_config.P_std + teacher_model.edm_config.P_mean).exp()
    
    print(f"üîç DEBUG: sigma.shape = {sigma.shape}, min = {sigma.min():.6f}, max = {sigma.max():.6f}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —á–∏—Å—Ç—ã–º –ª–∞—Ç–µ–Ω—Ç–∞–º
    noise = torch.randn_like(latents) * sigma
    noisy_latents = latents + noise
    
    print(f"üîç DEBUG: noisy_latents.shape = {noisy_latents.shape}, min = {noisy_latents.min():.6f}, max = {noisy_latents.max():.6f}")
    
    # Teacher: –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    with torch.no_grad():
        teacher_output = teacher_model.model_forward_wrapper(
            noisy_latents,
            sigma,
            text_embeddings,
            teacher_model.dit,
            mask_ratio=0.0
        )
        teacher_denoised = teacher_output['sample']
    
    print(f"üîç DEBUG: teacher_denoised.shape = {teacher_denoised.shape}, min = {teacher_denoised.min():.6f}, max = {teacher_denoised.max():.6f}")
    
    # Student: —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    student_output = student_model.model_forward_wrapper(
        noisy_latents,
        sigma,
        text_embeddings,
        student_model.dit,
        mask_ratio=0.0
    )
    student_denoised = student_output['sample']
    
    print(f"üîç DEBUG: student_denoised.shape = {student_denoised.shape}, min = {student_denoised.min():.6f}, max = {student_denoised.max():.6f}")
    
    # Consistency loss
    loss = nn.MSELoss()(student_denoised, teacher_denoised)
    
    print(f"üîç DEBUG: loss = {loss.item():.6f}")
    
    return loss

# =======================
# –ó–∞–ø—É—Å–∫ –æ—Ç–ª–∞–¥–∫–∏
# =======================
if __name__ == "__main__":
    print("üîç –û–¢–õ–ê–î–ö–ê —Å –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    if not os.path.exists("teacher_model_ready.pt") or not os.path.exists("student_model_ready.pt"):
        print("‚ùå –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python save_models.py")
        exit(1)
    
    # =======================
    # –ë–´–°–¢–†–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô
    # =======================
    print("‚ö° –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    try:
        teacher_model = torch.load("teacher_model_ready.pt", map_location="cpu")
        print("‚úÖ –£—á–∏—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ —Å–µ–∫—É–Ω–¥—ã!")
        
        student_model = torch.load("student_model_ready.pt", map_location="cpu")
        print("‚úÖ –°—Ç—É–¥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ —Å–µ–∫—É–Ω–¥—ã!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
        exit(1)

    # =======================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–∞—Ç—á)
    # =======================
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏...")
    latents_dir = r"C:\newTry2\train\datadir\latents_good"
    prompts_dir = r"C:\newTry2\train\datadir\prompts_good"
    
    dataset = LatentPromptDataset(latents_dir, prompts_dir)
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    latents, prompts = dataset[0]
    latents = latents.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –æ–¥–∏–Ω –±–∞—Ç—á: latents.shape = {latents.shape}")
    print(f"‚úÖ –ü—Ä–æ–º–ø—Ç: {prompts[:50]}...")
    
    # =======================
    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π —à–∞–≥
    # =======================
    print("\nüîç –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π —à–∞–≥...")
    try:
        text_embeddings = get_text_embeddings([prompts], teacher_model)
        loss = consistency_distillation_step_debug(latents, text_embeddings, teacher_model, student_model)
        
        print(f"\n‚úÖ –û—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä Loss: {loss.item():.6f}")
        
        if torch.isnan(loss):
            print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: Loss —Å–æ–¥–µ—Ä–∂–∏—Ç NaN!")
        else:
            print("‚úÖ Loss –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π (–Ω–µ NaN)")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ: {e}")
        import traceback
        traceback.print_exc()

