#!/usr/bin/env python3

import torch
import os
import sys
sys.path.append('/home/ubuntu/train/train')

from micro_diffusion.models.model import create_latent_diffusion
from micro_diffusion.models.model import UniversalTextEncoder
from diffusers import AutoencoderKL
import torchvision.transforms as T

def test_generation():
    device = torch.device('cuda')
    print('üé® –¢–ï–°–¢–ò–†–£–ï–ú –ì–ï–ù–ï–†–ê–¶–ò–Æ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:')
    print('=' * 50)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print('üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...')
    student_state_dict = torch.load('student_test_cd_fixed_text_encoder.pt', map_location=device)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    from micro_diffusion.models.dit import DiT
    student_model = DiT(
        dim=384,
        depth=12,
        head_dim=64,
        patch_mixer_dim=256,
        patch_mixer_depth=4,
        caption_channels=4096,
    )
    student_model.load_state_dict(student_state_dict)
    student_model.to(device, dtype=torch.float32)
    student_model.eval()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º VAE
    print('üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º VAE...')
    vae = AutoencoderKL.from_pretrained('stabilityai/sdxl-vae', torch_dtype=torch.float32)
    vae.to(device)
    vae.eval()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º Text Encoder
    print('üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º Text Encoder...')
    universal_text_encoder = UniversalTextEncoder(
        'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378', 
        dtype='bfloat16', 
        pretrained=True
    )

    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        'A beautiful sunset over mountains',
        'A cozy cabin in a snowy forest', 
        'A majestic dragon flying over a medieval castle'
    ]

    os.makedirs('test_fixed_text_encoder_outputs', exist_ok=True)

    with torch.no_grad():
        for i, prompt in enumerate(test_prompts):
            print(f'\nüìù –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: {prompt}')
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            text_embeddings = universal_text_encoder.encode_text(prompt).to(torch.float32)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–∞—Ç–µ–Ω—Ç—ã
            latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float32)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º (4 —à–∞–≥–∞)
            print(f'üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 4 —à–∞–≥–æ–≤...')
            for step in range(4):
                t = torch.ones(1, device=device, dtype=torch.float32) * (1.0 - step / 3.0)
                output = student_model(latents, t, text_embeddings)
                latents = output['sample'] if isinstance(output, dict) else output
                print(f'üîÑ –®–∞–≥ {step + 1}/4: t={t.item():.3f}')
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            latents_fp32 = latents.to(torch.float32)
            decoded_output = vae.decode(latents_fp32)
            decoded_image = decoded_output.sample if hasattr(decoded_output, 'sample') else decoded_output
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PIL
            image = T.ToPILImage()(decoded_image[0].cpu().clamp(-1, 1) * 0.5 + 0.5)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            filename = f'test_fixed_text_encoder_outputs/test_generated_{i+1}.png'
            image.save(filename)
            print(f'üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}')

    print('\nüé® –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!')
    print('üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: test_fixed_text_encoder_outputs/')

if __name__ == "__main__":
    test_generation()
