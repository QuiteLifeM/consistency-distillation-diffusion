#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç Consistency Distillation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ micro_diffusion
sys.path.append('/home/ubuntu/train/train/micro_diffusion')

def simple_consistency_test():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç Consistency Distillation"""
    print("üöÄ –ü–†–û–°–¢–û–ô –¢–ï–°–¢ CONSISTENCY DISTILLATION")
    print("=" * 50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é Student –º–æ–¥–µ–ª—å
    from micro_diffusion.models.dit import DiT
    
    student_model = DiT(
        input_size=64,
        patch_size=2,
        in_channels=4,
        hidden_size=1152,
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
        class_dropout_prob=0.1,
        num_classes=1000,
        learn_sigma=True,
        cond_drop_prob=0.1,
    )
    student_model.to(device)
    student_model.train()
    
    print("‚úÖ Student –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 1
    latent_size = 64
    channels = 4
    
    # x_0 - —Ü–µ–ª—å (—á–∏—Å—Ç—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã)
    x_0 = torch.randn(batch_size, channels, latent_size, latent_size, device=device)
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∑–∞–≥–ª—É—à–∫–∞)
    text_embeddings = torch.randn(batch_size, 77, 1152, device=device)
    
    print(f"üìä –†–∞–∑–º–µ—Ä—ã: x_0={x_0.shape}, text_embeddings={text_embeddings.shape}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)
    
    print("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    
    losses = []
    
    for iteration in range(10):  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–æ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
        optimizer.zero_grad()
        
        # 1. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π timestep
        t = torch.rand(batch_size, device=device)
        
        # 2. –ó–∞—à—É–º–∏–≤–∞–µ–º x_0 –¥–æ x_t
        noise = torch.randn_like(x_0)
        x_t = x_0 + noise * t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
        
        # 3. Student –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0 –∏–∑ x_t
        student_pred = student_model(x_t, t, text_embeddings)
        
        # 4. –û–°–ù–û–í–ù–û–ô LOSS - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ x_0!
        loss = F.mse_loss(student_pred, x_0)
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        losses.append(loss.item())
        
        if iteration % 2 == 0:
            print(f"  –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: Loss = {loss.item():.6f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"  –ù–∞—á–∞–ª—å–Ω—ã–π loss: {losses[0]:.6f}")
    print(f"  –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {losses[-1]:.6f}")
    
    improvement = (losses[0] - losses[-1]) / losses[0] * 100
    print(f"  –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.2f}%")
    
    if improvement > 5:
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è! Loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è")
    elif improvement > 0:
        print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ")
    else:
        print("‚ùå –ú–æ–¥–µ–ª—å –ù–ï –æ–±—É—á–∞–µ—Ç—Å—è!")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    print(f"\nüé® –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ...")
    
    with torch.no_grad():
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ —à—É–º–∞
        noise = torch.randn_like(x_0)
        t_test = torch.ones(batch_size, device=device) * 0.5
        
        student_pred = student_model(noise, t_test, text_embeddings)
        
        mse_to_target = F.mse_loss(student_pred, x_0).item()
        print(f"üéØ MSE –∫ —Ü–µ–ª–µ–≤–æ–º—É x_0: {mse_to_target:.6f}")
        
        if mse_to_target < 1.0:
            print("‚úÖ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0!")
        else:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0")
    
    print(f"\nüéâ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù!")
    return losses

if __name__ == "__main__":
    try:
        losses = simple_consistency_test()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

