{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc1cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 100/4000\n",
      "‚úÖ 200/4000\n",
      "‚úÖ 300/4000\n",
      "‚úÖ 400/4000\n",
      "‚úÖ 500/4000\n",
      "‚úÖ 600/4000\n",
      "‚úÖ 700/4000\n",
      "‚úÖ 800/4000\n",
      "‚úÖ 900/4000\n",
      "‚úÖ 1000/4000\n",
      "‚úÖ 1100/4000\n",
      "‚úÖ 1200/4000\n",
      "‚úÖ 1300/4000\n",
      "‚úÖ 1400/4000\n",
      "‚úÖ 1500/4000\n",
      "‚úÖ 1600/4000\n",
      "‚úÖ 1700/4000\n",
      "‚úÖ 1800/4000\n",
      "‚úÖ 1900/4000\n",
      "‚úÖ 2000/4000\n",
      "‚úÖ 2100/4000\n",
      "‚úÖ 2200/4000\n",
      "‚úÖ 2300/4000\n",
      "‚úÖ 2400/4000\n",
      "‚úÖ 2500/4000\n",
      "‚úÖ 2600/4000\n",
      "‚úÖ 2700/4000\n",
      "‚úÖ 2800/4000\n",
      "‚úÖ 2900/4000\n",
      "‚úÖ 3000/4000\n",
      "‚úÖ 3100/4000\n",
      "‚úÖ 3200/4000\n",
      "‚úÖ 3300/4000\n",
      "‚úÖ 3400/4000\n",
      "‚úÖ 3500/4000\n",
      "‚úÖ 3600/4000\n",
      "‚úÖ 3700/4000\n",
      "‚úÖ 3800/4000\n",
      "‚úÖ 3900/4000\n",
      "‚úÖ 4000/4000\n",
      "üéâ –í—Å–µ –ª–∞—Ç–µ–Ω—Ç—ã –∏ –ø—Ä–æ–º–ø—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "input_dir = \"dataset_stub_4k\"\n",
    "output_latent_dir = \"./datadir/latents\"\n",
    "output_prompt_dir = \"./datadir/prompts\"\n",
    "\n",
    "os.makedirs(output_latent_dir, exist_ok=True)\n",
    "os.makedirs(output_prompt_dir, exist_ok=True)\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n",
    "for i in range(1, 4001):\n",
    "    base_name = f\"image_{i:05d}\"\n",
    "    png_path = os.path.join(input_dir, f\"{base_name}.png\")\n",
    "    txt_path = os.path.join(input_dir, f\"{base_name}.txt\")\n",
    "\n",
    "    # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    image = Image.open(png_path).convert(\"RGB\")\n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: [0,255] ‚Üí [-1, 1]\n",
    "    image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).unsqueeze(0).float() / 127.5 - 1\n",
    "    image_tensor = image_tensor.to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ª–∞—Ç–µ–Ω—Ç\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(image_tensor).latent_dist.sample()\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∞—Ç–µ–Ω—Ç\n",
    "    torch.save(latents.cpu(), os.path.join(output_latent_dir, f\"{base_name}.pt\"))\n",
    "\n",
    "    # –ö–æ–ø–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç\n",
    "    shutil.copy(txt_path, os.path.join(output_prompt_dir, f\"{base_name}.txt\"))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"‚úÖ {i}/4000\")\n",
    "\n",
    "print(\"üéâ –í—Å–µ –ª–∞—Ç–µ–Ω—Ç—ã –∏ –ø—Ä–æ–º–ø—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80938472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ CLIP –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "def get_text_embeddings(prompts):\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    text_embeddings = text_model(**inputs).last_hidden_state.to(\"cuda\")\n",
    "    return text_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11465d08",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m prompts_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./datadir/prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# –ü–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLatentPromptDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mLatentPromptDataset.__init__\u001b[1;34m(self, latents_dir, prompts_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatents_dir \u001b[38;5;241m=\u001b[39m latents_dir\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts_dir \u001b[38;5;241m=\u001b[39m prompts_dir\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents_dir\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(prompts_dir))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫\n",
    "class LatentPromptDataset(Dataset):\n",
    "    def __init__(self, latents_dir, prompts_dir):\n",
    "        self.latents_dir = latents_dir\n",
    "        self.prompts_dir = prompts_dir\n",
    "        self.latent_files = sorted(os.listdir(latents_dir))  # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞\n",
    "        self.prompt_files = sorted(os.listdir(prompts_dir))  # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º\n",
    "        latent_path = os.path.join(self.latents_dir, self.latent_files[idx])\n",
    "        prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])\n",
    "\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–∞—Ç–µ–Ω—Ç—ã –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç\n",
    "        latents = torch.load(latent_path)\n",
    "        with open(prompt_path, 'r') as f:\n",
    "            prompt = f.read().strip()\n",
    "\n",
    "        return latents, prompt\n",
    "\n",
    "# –ü–∞–ø–∫–∞ —Å –ª–∞—Ç–µ–Ω—Ç–∞–º–∏ –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏\n",
    "latents_dir = \"./datadir/latents\"  # –ü–∞–ø–∫–∞ —Å –ª–∞—Ç–µ–Ω—Ç–∞–º–∏\n",
    "prompts_dir = \"./datadir/prompts\"  # –ü–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "dataset = LatentPromptDataset(latents_dir, prompts_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdefea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ CLIP –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "def get_text_embeddings(prompts):\n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ prompts - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫\n",
    "    if isinstance(prompts, str):  # –ï—Å–ª–∏ —ç—Ç–æ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞\n",
    "        prompts = [prompts]  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫\n",
    "\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    text_embeddings = text_model(**inputs).last_hidden_state.to(\"cuda\")\n",
    "    return text_embeddings\n",
    "\n",
    "# –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏\n",
    "images_dir = r\"C:\\newTry2\\dataset_stub_4k\"  # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "prompts_dir = r\"C:\\newTry2\\dataset_stub_4k\"  # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ 4D —Ç–µ–Ω–∑–æ—Ä\n",
    "def image_to_tensor(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # –ü–æ–¥–≥–æ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–¥ –º–æ–¥–µ–ª—å\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(\"cuda\")  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ GPU\n",
    "    return image_tensor\n",
    "\n",
    "# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫\n",
    "class LatentPromptDataset(Dataset):\n",
    "    def __init__(self, images_dir, prompts_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.prompts_dir = prompts_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.png')])\n",
    "        self.prompt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])\n",
    "\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç\n",
    "        image_tensor = image_to_tensor(image_path)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ 4D —Ç–µ–Ω–∑–æ—Ä\n",
    "        with open(prompt_path, 'r') as f:\n",
    "            prompt = f.read().strip()  # –ß–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø—Ä–æ–º–ø—Ç–∞\n",
    "\n",
    "        return image_tensor, prompt\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "dataset = LatentPromptDataset(images_dir, prompts_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc7c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU –ø–∞–º—è—Ç—å –¥–æ: 8.47 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\open_clip\\factory.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "C:\\Users\\Mark\\AppData\\Local\\Temp\\ipykernel_7772\\37373752.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"./micro_diffusion/trained_models/teacher.pt\", map_location=\"cpu\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—á–∏—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ CPU\n",
      "GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ: 8.43 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –ø–∞–º—è—Ç—å –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "print(f\"GPU –ø–∞–º—è—Ç—å –¥–æ: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è –Ω–∞ CPU\n",
    "teacher_model = create_latent_diffusion(\n",
    "    latent_res=64,\n",
    "    in_channels=4,\n",
    "    pos_interp_scale=2.0,\n",
    "    precomputed_latents=True,\n",
    "    # text_latents_key=\"caption_latents\", \n",
    "    dtype=\"float32\"  # –£–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
    ").to(\"cpu\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ —É—á–∏—Ç–µ–ª—è —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "teacher_model.dit.load_state_dict(\n",
    "    torch.load(\"./micro_diffusion/trained_models/teacher.pt\", map_location=\"cpu\"),\n",
    "    strict=False  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∫–ª—é—á–∞—Ö\n",
    ")\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (inference)\n",
    "teacher_model.eval()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "print(\"‚úÖ –£—á–∏—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ CPU\")\n",
    "print(f\"GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39bc6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_teacher_latents(image, text_embeddings):\n",
    "    with torch.no_grad():\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "        batch = {\n",
    "            \"image\": image,  # –ü–µ—Ä–µ–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–µ –ª–∞—Ç–µ–Ω—Ç—ã)\n",
    "            \"caption_latents\": text_embeddings  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "        }\n",
    "        \n",
    "        # –ü–æ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è\n",
    "        teacher_latents = teacher_model(batch)  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å\n",
    "    return teacher_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cba628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ 4D —Ç–µ–Ω–∑–æ—Ä\n",
    "def image_to_tensor(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # –ü–æ–¥–≥–æ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–¥ –º–æ–¥–µ–ª—å\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(\"cuda\")  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ GPU\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20377bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt  # –î–ª—è –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ GPU\n",
    "student_model = create_latent_diffusion(\n",
    "    latent_res=64,\n",
    "    in_channels=4,\n",
    "    pos_interp_scale=2.0,\n",
    "    dtype=\"float32\"  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ float32 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    ").to(\"cuda\")\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=1e-5)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —É—á–∏—Ç–µ–ª–µ–º –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–º\n",
    "def consistency_loss(student_latents, teacher_latents):\n",
    "    return nn.MSELoss()(student_latents, teacher_latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e6c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –õ–∏—Å—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ª–æ—Å—Å–∞\n",
    "losses = []\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞\n",
    "def train_student_model(dataloader, teacher_model, num_epochs):\n",
    "    student_model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # –°—É–º–º–∞ –ø–æ—Ç–µ—Ä—å –∑–∞ —ç–ø–æ—Ö—É\n",
    "        for i, (images, prompts) in enumerate(dataloader):  # –¢–µ–ø–µ—Ä—å images, –∞ –Ω–µ latents\n",
    "            images = images.to(\"cuda\")[:,0,...]\n",
    "            text_embeddings = get_text_embeddings(prompts).to(\"cuda\")\n",
    "            print(images.shape, text_embeddings.shape)\n",
    "            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª—å—é —É—á–∏—Ç–µ–ª—è (–Ω–∞ CPU)\n",
    "            teacher_latents = generate_teacher_latents(images.to(\"cpu\"), text_embeddings.to(\"cpu\"))\n",
    "\n",
    "            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª—å—é —Å—Ç—É–¥–µ–Ω—Ç–∞ (–Ω–∞ GPU)\n",
    "            student_latents = student_model(images, text_embeddings)\n",
    "\n",
    "            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å\n",
    "            loss = consistency_loss(student_latents, teacher_latents)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"–≠–ø–æ—Ö–∞ {epoch+1}, –ò—Ç–µ—Ä–∞—Ü–∏—è {i}, –ü–æ—Ç–µ—Ä–∏: {loss.item()}\")\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ç–µ—Ä–∏ –∑–∞ —ç–ø–æ—Ö—É\n",
    "        losses.append(epoch_loss / len(dataloader))  # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –∑–∞ —ç–ø–æ—Ö—É\n",
    "        print(f\"–≠–ø–æ—Ö–∞ {epoch+1}, –°—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "    print(\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d87586d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256]) torch.Size([16, 5, 512])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input height (32) doesn't match model (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# –ü—Ä–∏–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_student_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mtrain_student_model\u001b[1;34m(dataloader, teacher_model, num_epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape, text_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª—å—é —É—á–∏—Ç–µ–ª—è (–Ω–∞ CPU)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m teacher_latents \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_teacher_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª—å—é —Å—Ç—É–¥–µ–Ω—Ç–∞ (–Ω–∞ GPU)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m student_latents \u001b[38;5;241m=\u001b[39m student_model(images, text_embeddings)\n",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m, in \u001b[0;36mgenerate_teacher_latents\u001b[1;34m(image, text_embeddings)\u001b[0m\n\u001b[0;32m      4\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image,  \u001b[38;5;66;03m# –ü–µ—Ä–µ–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–µ –ª–∞—Ç–µ–Ω—Ç—ã)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_latents\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_embeddings  \u001b[38;5;66;03m# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–∞\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     }\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# –ü–æ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     teacher_latents \u001b[38;5;241m=\u001b[39m \u001b[43mteacher_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# –ü–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m teacher_latents\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\newTry2\\micro_diffusion\\micro_diffusion\\models\\model.py:137\u001b[0m, in \u001b[0;36mLatentDiffusion.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop_caption_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    133\u001b[0m     conditioning \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop_caption_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    134\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(conditioning\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 137\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medm_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_mask_ratio\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, latents, conditioning)\n",
      "File \u001b[1;32mc:\\newTry2\\micro_diffusion\\micro_diffusion\\models\\model.py:190\u001b[0m, in \u001b[0;36mLatentDiffusion.edm_loss\u001b[1;34m(self, x, y, mask_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m weight \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    185\u001b[0m     (sigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medm_config\u001b[38;5;241m.\u001b[39msigma_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[0;32m    186\u001b[0m     (sigma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medm_config\u001b[38;5;241m.\u001b[39msigma_data) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    187\u001b[0m )\n\u001b[0;32m    188\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandn_like(x) \u001b[38;5;241m*\u001b[39m sigma\n\u001b[1;32m--> 190\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward_wrapper(\n\u001b[0;32m    191\u001b[0m     x \u001b[38;5;241m+\u001b[39m n,\n\u001b[0;32m    192\u001b[0m     sigma,\n\u001b[0;32m    193\u001b[0m     y,\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdit,\n\u001b[0;32m    195\u001b[0m     mask_ratio\u001b[38;5;241m=\u001b[39mmask_ratio,\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    198\u001b[0m D_xn \u001b[38;5;241m=\u001b[39m model_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    199\u001b[0m loss \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m ((D_xn \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (N, C, H, W)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\newTry2\\micro_diffusion\\micro_diffusion\\models\\model.py:166\u001b[0m, in \u001b[0;36mLatentDiffusion.model_forward_wrapper\u001b[1;34m(self, x, sigma, y, model_forward_fxn, mask_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m c_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medm_config\u001b[38;5;241m.\u001b[39msigma_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m sigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m    164\u001b[0m c_noise \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m--> 166\u001b[0m out \u001b[38;5;241m=\u001b[39m model_forward_fxn(\n\u001b[0;32m    167\u001b[0m     (c_in \u001b[38;5;241m*\u001b[39m x)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[0;32m    168\u001b[0m     c_noise\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    169\u001b[0m     y,\n\u001b[0;32m    170\u001b[0m     mask_ratio\u001b[38;5;241m=\u001b[39mmask_ratio,\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m F_x \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    174\u001b[0m c_skip \u001b[38;5;241m=\u001b[39m c_skip\u001b[38;5;241m.\u001b[39mto(F_x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\newTry2\\micro_diffusion\\micro_diffusion\\models\\dit.py:564\u001b[0m, in \u001b[0;36mDiT.forward\u001b[1;34m(self, x, t, y, cfg, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_with_cfg(x, t, y, cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_without_cfg(x, t, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\newTry2\\micro_diffusion\\micro_diffusion\\models\\dit.py:479\u001b[0m, in \u001b[0;36mDiT.forward_without_cfg\u001b[1;34m(self, x, t, y, mask_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m--> 479\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed  \u001b[38;5;66;03m# (N, T, D), where T = H * W / patch_size ** 2\u001b[39;00m\n\u001b[0;32m    480\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_embedder(t\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[0;32m    482\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_embedder(y)  \u001b[38;5;66;03m# (N, 1, L, D)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\timm\\layers\\patch_embed.py:116\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_img_size:\n\u001b[1;32m--> 116\u001b[0m         \u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput height (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) doesn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt match model (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m).\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m         _assert(W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput width (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_img_pad:\n",
      "File \u001b[1;32mc:\\newTry2\\condaR\\envs\\sdxl-turbo-env\\lib\\site-packages\\torch\\__init__.py:2040\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhas_torch_function(\n\u001b[0;32m   2035\u001b[0m     (condition,)\n\u001b[0;32m   2036\u001b[0m ):\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m   2038\u001b[0m         _assert, (condition,), condition, message\n\u001b[0;32m   2039\u001b[0m     )\n\u001b[1;32m-> 2040\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[1;31mAssertionError\u001b[0m: Input height (32) doesn't match model (64)."
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "num_epochs = 10  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "train_student_model(dataloader, teacher_model, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ì—Ä–∞—Ñ–∏–∫ –ª–æ—Å—Å–∞\n",
    "plt.plot(range(1, num_epochs+1), losses)\n",
    "plt.xlabel(\"–≠–ø–æ—Ö–∏\")\n",
    "plt.ylabel(\"–°—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å\")\n",
    "plt.title(\"–ì—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ª–æ—Å—Å–∞\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (sdxl-turbo-env)",
   "language": "python",
   "name": "sdxl-turbo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
