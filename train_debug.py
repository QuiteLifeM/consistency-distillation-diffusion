import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è CPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# =======================
# Dataset –¥–ª—è –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤
# =======================
class LatentPromptDataset(Dataset):
    """Dataset –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    def __init__(self, latents_dir, prompts_dir):
        self.latents_dir = latents_dir
        self.prompts_dir = prompts_dir
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
        self.latent_files = sorted([f for f in os.listdir(latents_dir) if f.endswith('.pt')])
        self.prompt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])
        
        assert len(self.latent_files) == len(self.prompt_files), \
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–æ–≤ ({len(self.latent_files)}) != –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ ({len(self.prompt_files)})"
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.latent_files)} –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤")

    def __len__(self):
        return len(self.latent_files)

    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–∞—Ç–µ–Ω—Ç
        latent_path = os.path.join(self.latents_dir, self.latent_files[idx])
        latent = torch.load(latent_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])
        with open(prompt_path, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()
        
        return latent, prompt

# =======================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Consistency Distillation –Ω–∞ CPU (–æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è)
# =======================
def get_text_embeddings(prompts, model):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –º–æ–¥–µ–ª–∏
    tokenized = model.tokenizer.tokenize(prompts)
    device = next(model.parameters()).device
    input_ids = tokenized['input_ids'].to(device)
    
    with torch.no_grad():
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ text_encoder –º–æ–¥–µ–ª–∏
        text_embeddings = model.text_encoder.encode(input_ids)[0]
    
    return text_embeddings  # –û—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ (CPU)

def consistency_distillation_step_debug(latents, text_embeddings, teacher_model, student_model):
    """
    –û–¥–∏–Ω —à–∞–≥ Consistency Distillation –Ω–∞ CPU (–æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    """
    print(f"üîç DEBUG: latents shape: {latents.shape}, dtype: {latents.dtype}")
    print(f"üîç DEBUG: latents min: {latents.min():.6f}, max: {latents.max():.6f}")
    print(f"üîç DEBUG: latents has NaN: {torch.isnan(latents).any()}")
    print(f"üîç DEBUG: text_embeddings shape: {text_embeddings.shape}, dtype: {text_embeddings.dtype}")
    print(f"üîç DEBUG: text_embeddings min: {text_embeddings.min():.6f}, max: {text_embeddings.max():.6f}")
    print(f"üîç DEBUG: text_embeddings has NaN: {torch.isnan(text_embeddings).any()}")
    
    batch_size = latents.shape[0]
    
    # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞ –∏–∑ EDM —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    rnd_normal = torch.randn([batch_size, 1, 1, 1], device=latents.device)
    sigma = (rnd_normal * teacher_model.edm_config.P_std + teacher_model.edm_config.P_mean).exp()
    
    print(f"üîç DEBUG: sigma shape: {sigma.shape}, dtype: {sigma.dtype}, min: {sigma.min():.6f}, max: {sigma.max():.6f}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —á–∏—Å—Ç—ã–º –ª–∞—Ç–µ–Ω—Ç–∞–º
    noise = torch.randn_like(latents) * sigma
    noisy_latents = latents + noise
    
    print(f"üîç DEBUG: noisy_latents shape: {noisy_latents.shape}, dtype: {noisy_latents.dtype}")
    print(f"üîç DEBUG: noisy_latents min: {noisy_latents.min():.6f}, max: {noisy_latents.max():.6f}")
    
    # Teacher: –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞ —Å —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞
    with torch.no_grad():
        print("üîç DEBUG: –ó–∞–ø—É—Å–∫–∞–µ–º teacher...")
        teacher_output = teacher_model.model_forward_wrapper(
            noisy_latents,
            sigma,
            text_embeddings,
            teacher_model.dit,
            mask_ratio=0.0
        )
        teacher_denoised = teacher_output['sample']
        print(f"üîç DEBUG: teacher_denoised shape: {teacher_denoised.shape}, dtype: {teacher_denoised.dtype}")
        print(f"üîç DEBUG: teacher_denoised min: {teacher_denoised.min():.6f}, max: {teacher_denoised.max():.6f}")
    
    # Student: —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    print("üîç DEBUG: –ó–∞–ø—É—Å–∫–∞–µ–º student...")
    student_output = student_model.model_forward_wrapper(
        noisy_latents,
        sigma,
        text_embeddings,
        student_model.dit,
        mask_ratio=0.0
    )
    student_denoised = student_output['sample']
    print(f"üîç DEBUG: student_denoised shape: {student_denoised.shape}, dtype: {student_denoised.dtype}")
    print(f"üîç DEBUG: student_denoised min: {student_denoised.min():.6f}, max: {student_denoised.max():.6f}")
    
    # Consistency loss: —Å—Ç—É–¥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ –∂–µ, —á—Ç–æ –∏ —É—á–∏—Ç–µ–ª—å
    loss = nn.MSELoss()(student_denoised, teacher_denoised)
    print(f"üîç DEBUG: loss: {loss.item():.6f}")
    
    return loss

# =======================
# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª (–æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è)
# =======================
def custom_collate(batch):
    """
    –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–∞—Ç–µ–Ω—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ–º–ø—Ç—ã
    """
    latents_list = []
    prompts_list = []
    
    for latent, prompt in batch:
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ª–∞—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å [C, H, W]
        if latent.dim() == 4 and latent.shape[0] == 1:
            latent = latent.squeeze(0)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é batch dimension
        
        latents_list.append(latent)
        prompts_list.append(prompt)
    
    # –°—Ç–∞–∫–∞–µ–º –ª–∞—Ç–µ–Ω—Ç—ã –≤ –±–∞—Ç—á [B, C, H, W]
    latents_batch = torch.stack(latents_list, dim=0)
    
    return latents_batch, prompts_list

# =======================
# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
# =======================
if __name__ == "__main__":
    print("üîç –û–¢–õ–ê–î–û–ß–ù–ê–Ø –í–ï–†–°–ò–Ø - —Ç–æ–ª—å–∫–æ 1 –±–∞—Ç—á –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏")
    
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    latents_dir = r"C:\newTry2\train\datadir\latents_good"
    prompts_dir = r"C:\newTry2\train\datadir\prompts_good"
    
    # =======================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —É—á–∏—Ç–µ–ª—è –∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ CPU
    # =======================
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —É—á–∏—Ç–µ–ª—è –Ω–∞ CPU...")
    try:
        teacher_model = create_latent_diffusion(
            latent_res=64,
            in_channels=4,
            pos_interp_scale=2.0,
            precomputed_latents=False,
            dtype="float32"
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞ CPU...")
        teacher_model = teacher_model.to("cpu")
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–∞ CPU")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        exit(1)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —É—á–∏—Ç–µ–ª—è
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —É—á–∏—Ç–µ–ª—è...")
    try:
        teacher_weights = torch.load("./micro_diffusion/trained_models/teacher.pt", map_location="cpu")
        teacher_model.dit.load_state_dict(teacher_weights, strict=False)
        teacher_model.eval()
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—á–∏—Ç–µ–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤: {e}")
        exit(1)

    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ CPU...")
    student_model = create_latent_diffusion(
        latent_res=64,
        in_channels=4,
        pos_interp_scale=2.0,
        precomputed_latents=False,
        dtype="float32"
    ).to("cpu")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—É–¥–µ–Ω—Ç–∞ –≤–µ—Å–∞–º–∏ —É—á–∏—Ç–µ–ª—è
    student_model.dit.load_state_dict(teacher_weights, strict=False)
    student_model.train()
    print("‚úÖ –ú–æ–¥–µ–ª—å —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤–µ—Å–∞–º–∏ —É—á–∏—Ç–µ–ª—è –Ω–∞ CPU")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä (—Ç–æ–ª—å–∫–æ 1 –±–∞—Ç—á –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    dataset = LatentPromptDataset(latents_dir, prompts_dir)
    dataloader = DataLoader(
        dataset, 
        batch_size=1,
        shuffle=True, 
        num_workers=0,  # –û—Ç–∫–ª—é—á–∞–µ–º multiprocessing –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        collate_fn=custom_collate
    )
    print(f"‚úÖ DataLoader —Å–æ–∑–¥–∞–Ω: {len(dataset)} —Å—ç–º–ø–ª–æ–≤, batch_size=1\n")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á...")
    for i, (latents, prompts) in enumerate(dataloader):
        if i >= 1:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
            break
            
        print(f"\n{'='*60}")
        print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞—Ç—á {i}")
        print(f"{'='*60}")
        
        try:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –ª–∞—Ç–µ–Ω—Ç—ã –Ω–∞ CPU
            latents = latents.float()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–æ–≤
            if latents.dim() == 3:
                latents = latents.unsqueeze(0)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–∞—Ç–µ–Ω—Ç—ã –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            assert latents.shape[1] == 4, f"–û–∂–∏–¥–∞–µ—Ç—Å—è 4 –∫–∞–Ω–∞–ª–∞, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[1]}"
            assert latents.shape[2] == 64 and latents.shape[3] == 64, \
                f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä 64x64, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[2]}x{latents.shape[3]}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if torch.isnan(latents).any():
                print(f"‚ùå –ò–°–•–û–î–ù–´–ï –õ–ê–¢–ï–ù–¢–´ –°–û–î–ï–†–ñ–ê–¢ NaN!")
                print(f"   latents min: {latents.min():.6f}, max: {latents.max():.6f}")
                print(f"   latents has NaN: {torch.isnan(latents).any()}")
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            text_embeddings = get_text_embeddings(prompts, teacher_model)
            
            # Consistency distillation step –Ω–∞ CPU
            loss = consistency_distillation_step_debug(
                latents, text_embeddings, teacher_model, student_model
            )
            
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ! Loss: {loss.item():.6f}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nüîç –û—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
