import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è CPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# =======================
# Dataset –¥–ª—è –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤
# =======================
class LatentPromptDataset(Dataset):
    """Dataset –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    def __init__(self, latents_dir, prompts_dir, preload=True):
        self.latents_dir = latents_dir
        self.prompts_dir = prompts_dir
        self.preload = preload
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ñ–∞–π–ª–æ–≤
        self.latent_files = sorted([f for f in os.listdir(latents_dir) if f.endswith('.pt')])
        self.prompt_files = sorted([f for f in os.listdir(prompts_dir) if f.endswith('.txt')])
        
        assert len(self.latent_files) == len(self.prompt_files), \
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–æ–≤ ({len(self.latent_files)}) != –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ ({len(self.prompt_files)})"
        
        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if preload:
            print("‚ö° –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å...")
            self.latents_cache = []
            self.prompts_cache = []
            
            for i in tqdm(range(len(self.latent_files)), desc="–ó–∞–≥—Ä—É–∑–∫–∞ –ª–∞—Ç–µ–Ω—Ç–æ–≤"):
                latent_path = os.path.join(self.latents_dir, self.latent_files[i])
                latent = torch.load(latent_path)
                self.latents_cache.append(latent)
                
                prompt_path = os.path.join(self.prompts_dir, self.prompt_files[i])
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    prompt = f.read().strip()
                self.prompts_cache.append(prompt)
            
            print(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.latents_cache)} –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤")
        else:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.latent_files)} –ª–∞—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)")

    def __len__(self):
        return len(self.latent_files)

    def __getitem__(self, idx):
        if self.preload:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            return self.latents_cache[idx], self.prompts_cache[idx]
        else:
            # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            latent_path = os.path.join(self.latents_dir, self.latent_files[idx])
            latent = torch.load(latent_path)
            
            prompt_path = os.path.join(self.prompts_dir, self.prompt_files[idx])
            with open(prompt_path, 'r', encoding='utf-8') as f:
                prompt = f.read().strip()
            
            return latent, prompt

# =======================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Consistency Distillation
# =======================
def get_text_embeddings(prompts, model):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"""
    tokenized = model.tokenizer.tokenize(prompts)
    device = next(model.parameters()).device
    input_ids = tokenized['input_ids'].to(device)
    
    with torch.no_grad():
        text_embeddings = model.text_encoder.encode(input_ids)[0]
    
    return text_embeddings

def consistency_distillation_step(latents, text_embeddings, teacher_model, student_model):
    """–û–¥–∏–Ω —à–∞–≥ Consistency Distillation"""
    batch_size = latents.shape[0]
    
    # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞ –∏–∑ EDM —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    rnd_normal = torch.randn([batch_size, 1, 1, 1], device=latents.device)
    sigma = (rnd_normal * teacher_model.edm_config.P_std + teacher_model.edm_config.P_mean).exp()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —á–∏—Å—Ç—ã–º –ª–∞—Ç–µ–Ω—Ç–∞–º
    noise = torch.randn_like(latents) * sigma
    noisy_latents = latents + noise
    
    # Teacher: –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    with torch.no_grad():
        teacher_output = teacher_model.model_forward_wrapper(
            noisy_latents,
            sigma,
            text_embeddings,
            teacher_model.dit,
            mask_ratio=0.0
        )
        teacher_denoised = teacher_output['sample']
    
    # Student: —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞
    student_output = student_model.model_forward_wrapper(
        noisy_latents,
        sigma,
        text_embeddings,
        student_model.dit,
        mask_ratio=0.0
    )
    student_denoised = student_output['sample']
    
    # Consistency loss
    loss = nn.MSELoss()(student_denoised, teacher_denoised)
    
    return loss

# =======================
# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª
# =======================
def custom_collate(batch):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π"""
    latents_list = []
    prompts_list = []
    
    for latent, prompt in batch:
        if latent.dim() == 4 and latent.shape[0] == 1:
            latent = latent.squeeze(0)
        
        latents_list.append(latent)
        prompts_list.append(prompt)
    
    latents_batch = torch.stack(latents_list, dim=0)
    return latents_batch, prompts_list

def train_consistency_distillation(dataloader, teacher_model, student_model, optimizer, num_epochs=5):
    """–û–±—É—á–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Consistency Distillation"""
    losses_history = []
    
    print(f"\n{'='*60}")
    print(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {num_epochs} —ç–ø–æ—Ö, {len(dataloader)} –±–∞—Ç—á–µ–π –Ω–∞ —ç–ø–æ—Ö—É")
    print(f"üöÄ –ë–´–°–¢–†–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: batch_size=1, num_workers=0, float32")
    print(f"‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: ~{len(dataloader) * 1.2 / 3600:.1f} —á–∞—Å–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É")
    print(f"{'='*60}\n")

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        student_model.train()
        num_batches = 0
        
        for i, (latents, prompts) in enumerate(tqdm(dataloader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}", unit="–±–∞—Ç—á")):
            try:
                latents = latents.float()
                
                if latents.dim() == 3:
                    latents = latents.unsqueeze(0)
                
                assert latents.shape[1] == 4, f"–û–∂–∏–¥–∞–µ—Ç—Å—è 4 –∫–∞–Ω–∞–ª–∞, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[1]}"
                assert latents.shape[2] == 64 and latents.shape[3] == 64, \
                    f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä 64x64, –ø–æ–ª—É—á–µ–Ω–æ {latents.shape[2]}x{latents.shape[3]}"
                
                text_embeddings = get_text_embeddings(prompts, teacher_model)
                loss = consistency_distillation_step(latents, text_embeddings, teacher_model, student_model)
                
                if torch.isnan(loss):
                    print(f"‚ö†Ô∏è NaN loss –≤ –±–∞—Ç—á–µ {i}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                    continue
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(student_model.dit.parameters(), max_norm=1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
                if i % 10 == 0:
                    tqdm.write(f"Loss: {loss.item():.6f}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}: {e}")
                continue
        
        if num_batches > 0:
            avg_epoch_loss = epoch_loss / num_batches
            losses_history.append(avg_epoch_loss)
            
            print(f"\n{'='*60}")
            print(f"–≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –°—Ä–µ–¥–Ω–∏–π Loss: {avg_epoch_loss:.6f}")
            print(f"{'='*60}\n")
            
            checkpoint_path = f"student_checkpoint_epoch_{epoch+1}.pt"
            torch.save(student_model.dit.state_dict(), checkpoint_path)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}\n")
        else:
            print(f"‚ö†Ô∏è –≠–ø–æ—Ö–∞ {epoch+1}: –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –±–∞—Ç—á–µ–π!")
    
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return losses_history

# =======================
# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
# =======================
if __name__ == "__main__":
    print("üöÄ –ë–´–°–¢–†–ê–Ø –ó–ê–ì–†–£–ó–ö–ê Consistency Distillation")
    print("‚ö° –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∑–∞ —Å–µ–∫—É–Ω–¥—ã!")
    print("üõ°Ô∏è –°–¢–ê–ë–ò–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø: batch_size=1 –¥–ª—è Windows")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    if not os.path.exists("teacher_model_ready.pt") or not os.path.exists("student_model_ready.pt"):
        print("‚ùå –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python save_models.py")
        exit(1)
    
    # =======================
    # –ë–´–°–¢–†–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô
    # =======================
    print("‚ö° –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    try:
        teacher_model = torch.load("teacher_model_ready.pt", map_location="cpu")
        print("‚úÖ –£—á–∏—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ —Å–µ–∫—É–Ω–¥—ã!")
        
        student_model = torch.load("student_model_ready.pt", map_location="cpu")
        print("‚úÖ –°—Ç—É–¥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ —Å–µ–∫—É–Ω–¥—ã!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª–∏: python save_models.py")
        exit(1)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞
    optimizer = optim.Adam(student_model.dit.parameters(), lr=1e-5)
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å lr={1e-5}\n")
    
    # =======================
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # =======================
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    latents_dir = r"C:\newTry2\train\datadir\latents_good"
    prompts_dir = r"C:\newTry2\train\datadir\prompts_good"
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∂–∏–º –∑–∞–≥—Ä—É–∑–∫–∏
    print("‚ö° –†–µ–∂–∏–º—ã –∑–∞–≥—Ä—É–∑–∫–∏:")
    print("1. –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ (–±—ã—Å—Ç—Ä–æ, –Ω–æ –º–Ω–æ–≥–æ RAM)")
    print("2. –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –º–∞–ª–æ RAM)")
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    preload_mode = True
    print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: {'–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞' if preload_mode else '–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞'}")
    
    dataset = LatentPromptDataset(latents_dir, prompts_dir, preload=preload_mode)
    dataloader = DataLoader(
        dataset, 
        batch_size=1,  # –í–µ—Ä–Ω—É–ª–∏ –∫ 1 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        shuffle=True, 
        num_workers=0,
        collate_fn=custom_collate
    )
    print(f"‚úÖ DataLoader —Å–æ–∑–¥–∞–Ω: {len(dataset)} —Å—ç–º–ø–ª–æ–≤, batch_size=1\n")
    
    # =======================
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    # =======================
    num_epochs = 5
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö...")
    print(f"‚è±Ô∏è  –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: ~{len(dataloader) * 1.2 * num_epochs / 3600:.1f} —á–∞—Å–æ–≤")
    print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ–∫–∞–∂–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ")
    
    losses = train_consistency_distillation(
        dataloader, teacher_model, student_model, optimizer, num_epochs
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    final_model_path = "student_final_fast_load.pt"
    torch.save(student_model.dit.state_dict(), final_model_path)
    print(f"\nüíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
    
    # –ì—Ä–∞—Ñ–∏–∫ –ª–æ—Å—Å–∞
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, num_epochs + 1), losses, marker='o', linewidth=2)
    plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Consistency Distillation Training Loss (Fast Load)', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.savefig('training_loss_fast_load.png', dpi=150, bbox_inches='tight')
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: training_loss_fast_load.png")
