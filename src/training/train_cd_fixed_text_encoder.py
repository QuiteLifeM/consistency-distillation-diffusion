import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import psutil

sys.path.append('/home/ubuntu/train/train/micro_diffusion')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion
from micro_diffusion.models.dit import MicroDiT_XL_2, MicroDiT_Tiny_2
from micro_diffusion.micro_diffusion.models.utils import UniversalTextEncoder, UniversalTokenizer
from create_proper_dataset import ProperDataset

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º VAE
from diffusers import AutoencoderKL

def get_memory_info():
    """–ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_reserved = torch.cuda.memory_reserved() / 1024**3
        gpu_free = gpu_memory - gpu_allocated
        return {
            'gpu_total': gpu_memory,
            'gpu_allocated': gpu_allocated,
            'gpu_reserved': gpu_reserved,
            'gpu_free': gpu_free
        }
    return {}

def log_memory_usage(iteration, epoch, prefix=""):
    """–õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    memory_info = get_memory_info()
    if memory_info:
        print(f"{prefix} –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}, –≠–ø–æ—Ö–∞ {epoch}")
        print(f"  GPU: {memory_info['gpu_allocated']:.1f}GB / {memory_info['gpu_total']:.1f}GB (—Å–≤–æ–±–æ–¥–Ω–æ: {memory_info['gpu_free']:.1f}GB)")
        print(f"  GPU Reserved: {memory_info['gpu_reserved']:.1f}GB")

def load_models(device="cuda"):
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú Text Encoder"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...")
    
    # Teacher –º–æ–¥–µ–ª—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º create_latent_diffusion –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    print("üß† –ó–∞–≥—Ä—É–∂–∞–µ–º Teacher —Å –ü–†–ê–í–ò–õ–¨–ù–û–ô –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π...")
    
    # –°–æ–∑–¥–∞–µ–º Teacher —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —á–µ—Ä–µ–∑ create_latent_diffusion
    teacher_model = create_latent_diffusion(
        latent_res=64,  # ‚úÖ Teacher –æ–∂–∏–¥–∞–µ—Ç 64x64 –ª–∞—Ç–µ–Ω—Ç—ã (–∫–∞–∫ –≤ checkpoint)
        in_channels=4, 
        pos_interp_scale=2.0,
        precomputed_latents=True,  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã
        dtype="bfloat16"
    ).to("cpu")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ Teacher (FID 12.66)
    pretrained_path = "/home/ubuntu/train/train/micro_diffusion/pretrained_models/dit_4_channel_37M_real_and_synthetic_data.pt"
    
    try:
        print(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∏–∑: {os.path.basename(pretrained_path)}")
        teacher_weights = torch.load(pretrained_path, map_location="cpu")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (strict=False –¥–ª—è MoE)
        teacher_model.dit.load_state_dict(teacher_weights, strict=False)
        print("‚úÖ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ Teacher –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        print("üéØ Teacher: FID 12.66 (37M –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤: {e}")
        print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é Teacher")
    
    teacher_model.eval()
    print("‚úÖ Teacher –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ CPU —Å –ü–†–ê–í–ò–õ–¨–ù–û–ô –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π")
    
    # Student –º–æ–¥–µ–ª—å - MicroDiT_Tiny_2 –Ω–∞ GPU (–æ–±—É—á–∞–µ—Ç—Å—è)
    print("üéì –ó–∞–≥—Ä—É–∂–∞–µ–º Student (DiT-Tiny) –Ω–∞ GPU...")
    student_model = MicroDiT_Tiny_2(
        input_size=64,  # ‚úÖ –¢–æ—Ç –∂–µ —Ä–∞–∑–º–µ—Ä, —á—Ç–æ —É Teacher (64x64)
        caption_channels=1024,  # ‚úÖ –¢–æ—Ç –∂–µ —Ä–∞–∑–º–µ—Ä, —á—Ç–æ —É Teacher
        pos_interp_scale=1.0,
        in_channels=4
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Student —Å–ª—É—á–∞–π–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
    def init_weights(m):
        if isinstance(m, torch.nn.Linear):
            if m.weight is not None:
                torch.nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                torch.nn.init.zeros_(m.bias)
        elif isinstance(m, torch.nn.LayerNorm):
            if m.weight is not None:
                torch.nn.init.ones_(m.weight)
            if m.bias is not None:
                torch.nn.init.zeros_(m.bias)
    
    student_model.apply(init_weights)
    print("‚úÖ Student –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–ª—É—á–∞–π–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏")
    
    student_model.to(device, dtype=torch.float32)
    student_model.train()
    
    # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–ê–ú–Ø–¢–ò –∏–∑ Hugging Face Diffusers
    # 1. Channels Last memory format (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
    student_model = student_model.to(memory_format=torch.channels_last)
    print("‚úÖ Channels Last –≤–∫–ª—é—á–µ–Ω –¥–ª—è Student")
    
    # 2. Memory Efficient Attention (xFormers) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è
    try:
        import xformers
        if hasattr(student_model, 'enable_xformers_memory_efficient_attention'):
            student_model.enable_xformers_memory_efficient_attention()
            print("‚úÖ xFormers Memory Efficient Attention –≤–∫–ª—é—á–µ–Ω")
        else:
            # Fallback: sliced attention
            if hasattr(student_model, 'enable_attention_slicing'):
                student_model.enable_attention_slicing(1)
                print("‚úÖ Sliced attention –≤–∫–ª—é—á–µ–Ω (fallback)")
    except ImportError:
        print("‚ö†Ô∏è xFormers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º sliced attention")
        # Fallback: sliced attention
        if hasattr(student_model, 'enable_attention_slicing'):
            student_model.enable_attention_slicing(1)
            print("‚úÖ Sliced attention –≤–∫–ª—é—á–µ–Ω (fallback)")
    except Exception as e:
        print(f"‚ö†Ô∏è xFormers –æ—à–∏–±–∫–∞: {e}")
        # Fallback: sliced attention
        if hasattr(student_model, 'enable_attention_slicing'):
            student_model.enable_attention_slicing(1)
            print("‚úÖ Sliced attention –≤–∫–ª—é—á–µ–Ω (fallback)")
    
    print("‚úÖ Student (DiT-Tiny) –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ GPU —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø–∞–º—è—Ç–∏")
    
    # 3. Gradient Checkpointing –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    # if hasattr(student_model, 'enable_gradient_checkpointing'):
    #     student_model.enable_gradient_checkpointing()
    #     print("‚úÖ Gradient Checkpointing –≤–∫–ª—é—á–µ–Ω (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
    # else:
    #     print("‚ö†Ô∏è Gradient Checkpointing –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    print("‚úÖ Gradient Checkpointing –æ—Ç–∫–ª—é—á–µ–Ω (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)")
    
    # –°–æ–∑–¥–∞–µ–º –ü–†–ê–í–ò–õ–¨–ù–´–ô —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ (–∫–∞–∫ —É Teacher)
    print("üîß –°–æ–∑–¥–∞–µ–º –ü–†–ê–í–ò–õ–¨–ù–´–ô —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫...")
    text_encoder = UniversalTextEncoder(
        'openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378',  # –¢–æ—Ç –∂–µ, —á—Ç–æ —É Teacher!
        dtype='bfloat16',
        pretrained=True
    ).to(device)  # ‚úÖ –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU
    tokenizer = UniversalTokenizer('openclip:hf-hub:apple/DFN5B-CLIP-ViT-H-14-378')
    
    print("‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô Text Encoder –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ GPU (–∫–∞–∫ —É Teacher)")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º VAE –¥–ª—è Student (—Ç–æ—Ç –∂–µ, —á—Ç–æ —É Teacher)
    print("üîß –ó–∞–≥—Ä—É–∂–∞–µ–º VAE –¥–ª—è Student...")
    vae = AutoencoderKL.from_pretrained(
        "stabilityai/sd-vae-ft-mse",
        torch_dtype=torch.float32
    ).to(device)  # VAE –Ω–∞ GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    
    # üöÄ VAE –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ü–ê–ú–Ø–¢–ò
    # 1. Sliced VAE decode –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    if hasattr(vae, 'enable_slicing'):
        vae.enable_slicing()
        print("‚úÖ Sliced VAE decode –≤–∫–ª—é—á–µ–Ω")
    
    # 2. Tiled VAE –¥–ª—è –±–æ–ª—å—à–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if hasattr(vae, 'enable_tiling'):
        vae.enable_tiling()
        print("‚úÖ Tiled VAE –≤–∫–ª—é—á–µ–Ω")
    
    print("‚úÖ VAE –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è Student —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø–∞–º—è—Ç–∏")
    
    print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    return teacher_model, student_model, text_encoder, tokenizer, vae

def consistency_distillation_step(image_path, prompt, teacher_model, student_model, text_encoder, tokenizer, vae, device="cuda"):
    """
    –ù–ê–°–¢–û–Ø–©–ò–ô Consistency Distillation:
    1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∫–æ–¥–∏—Ä—É–µ–º –≤ –ª–∞—Ç–µ–Ω—Ç—ã (x_0)
    2. –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –î–í–ê —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–∏ (t1, t2) –∏–∑ –û–î–ù–û–ô —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
    4. –ó–∞—à—É–º–ª—è–µ–º –ø–æ-—Ä–∞–∑–Ω–æ–º—É: x_t1, x_t2
    5. Student –¥–æ–ª–∂–µ–Ω –¥–∞—Ç—å –û–î–ò–ù–ê–ö–û–í–´–ô x_0 –¥–ª—è –û–ë–û–ò–•!
    6. Consistency Loss: F.mse_loss(student(x_t1), student(x_t2))
    """
    try:
        # üöÄ –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        cache_key = f"{image_path}_{prompt}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if not hasattr(consistency_distillation_step, 'cache'):
            consistency_distillation_step.cache = {}
        
        if cache_key in consistency_distillation_step.cache:
            latents, text_embeddings = consistency_distillation_step.cache[cache_key]
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∏–∫—Å–µ–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            from PIL import Image
            import torchvision.transforms as transforms
            
            image = Image.open(image_path).convert('RGB')
            transform = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5])
            ])
            image_tensor = transform(image).unsqueeze(0).to(device)
            
            # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –ª–∞—Ç–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ VAE (—ç—Ç–æ –Ω–∞—à x_0)
            with torch.no_grad():
                # VAE –Ω–∞ GPU, –¥–∞–Ω–Ω—ã–µ —É–∂–µ –Ω–∞ GPU
                latents = vae.encode(image_tensor).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
            
            # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ Text Encoder
            tokenized = tokenizer.tokenize(prompt)
            text_embeddings = text_encoder.encode(tokenized['input_ids'].to(device))[0].to(device, dtype=torch.float32)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (–¥–ª—è –ø–µ—Ä–≤—ã—Ö 1000 –æ–±—Ä–∞–∑—Ü–æ–≤)
            if len(consistency_distillation_step.cache) < 1000:
                consistency_distillation_step.cache[cache_key] = (latents, text_embeddings)
        
        # üéØ –ù–ê–°–¢–û–Ø–©–ò–ô CD: –î–í–ê —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –û–î–ù–û–ô —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        t1 = torch.rand(1, device=device, dtype=torch.float32)
        t2 = torch.rand(1, device=device, dtype=torch.float32)
        
        # –°–æ–∑–¥–∞–µ–º –î–í–ê —Ä–∞–∑–Ω—ã—Ö —à—É–º–∞ –¥–ª—è –û–î–ù–û–ô —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        noise1 = torch.randn_like(latents)
        noise2 = torch.randn_like(latents)
        
        # –ó–∞—à—É–º–ª—è–µ–º –ø–æ-—Ä–∞–∑–Ω–æ–º—É (–û–î–ù–ê —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è!)
        x_t1 = latents + t1 * noise1  # x_t1 –∏–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        x_t2 = latents + t2 * noise2  # x_t2 –∏–∑ —Ç–æ–π –∂–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        
        # üéØ CONSISTENCY: Student –¥–æ–ª–∂–µ–Ω –¥–∞—Ç—å –û–î–ò–ù–ê–ö–û–í–´–ô x_0!
        # Student - —ç—Ç–æ –Ω–∞—à–∞ consistency function f(x_t, t) -> x_0
        student_x0_1 = student_model(x_t1, t1, text_embeddings)
        student_x0_1 = student_x0_1['sample'] if isinstance(student_x0_1, dict) else student_x0_1
        
        student_x0_2 = student_model(x_t2, t2, text_embeddings)
        student_x0_2 = student_x0_2['sample'] if isinstance(student_x0_2, dict) else student_x0_2
        
        # üéØ CONSISTENCY LOSS: Student –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º!
        # f(x_t1, t1) –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–≤–Ω–æ f(x_t2, t2) –¥–ª—è –æ–¥–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        consistency_loss = F.mse_loss(student_x0_1, student_x0_2)
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø–æ–Ω–∏–º–∞–µ–º, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç
        if not hasattr(consistency_distillation_step, 'debug_count'):
            consistency_distillation_step.debug_count = 0
        
        if consistency_distillation_step.debug_count < 5:  # –ü–µ—Ä–≤—ã–µ 5 –∏—Ç–µ—Ä–∞—Ü–∏–π
            print(f"üîç DEBUG –∏—Ç–µ—Ä–∞—Ü–∏—è {consistency_distillation_step.debug_count}:")
            print(f"  Student output 1 mean: {student_x0_1.mean():.6f}")
            print(f"  Student output 2 mean: {student_x0_2.mean():.6f}")
            print(f"  Difference mean: {torch.abs(student_x0_1 - student_x0_2).mean():.6f}")
            print(f"  Loss: {consistency_loss.item():.6f}")
            print(f"  t1: {t1.item():.6f}, t2: {t2.item():.6f}")
            consistency_distillation_step.debug_count += 1
        
        return consistency_loss
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ CD —à–∞–≥–µ: {e}")
        return None

def train_cd_fixed_text_encoder():
    """CD –æ–±—É—á–µ–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú Text Encoder"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    teacher_model, student_model, text_encoder, tokenizer, vae = load_models(device)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –ü–ò–ö–°–ï–õ–¨–ù–´–ú–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú–ò –∏ –ü–†–û–ú–ü–¢–ê–ú–ò
    print("\nüìä –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –ü–ò–ö–°–ï–õ–¨–ù–´–ú–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú–ò –∏ –ü–†–û–ú–ü–¢–ê–ú–ò...")
    data_dir = "/home/ubuntu/train/train/dataset_sdxl_turbo"  # –ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    
    class PixelDataset:
        def __init__(self, data_dir, device, preload_to_ram=True):
            self.data_dir = data_dir
            self.device = device
            self.preload_to_ram = preload_to_ram
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ PNG —Ñ–∞–π–ª—ã
            all_files = os.listdir(data_dir)
            self.image_files = sorted([f for f in all_files if f.endswith('.png')])
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(self.image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(self.image_files)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∞–π–ª–æ–≤
            for img_file in self.image_files[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5
                txt_file = img_file.replace('.png', '.txt')
                if not os.path.exists(os.path.join(data_dir, txt_file)):
                    print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ—Ç —Ñ–∞–π–ª–∞ {txt_file} –¥–ª—è {img_file}")
            
            # üöÄ –ü–†–ï–î–ó–ê–ì–†–£–ó–ö–ê –í –û–ó–£ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            if preload_to_ram:
                print("üöÄ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –û–ó–£ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è...")
                self.images_ram = {}
                self.prompts_ram = {}
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –û–ó–£ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
                for i, img_file in enumerate(self.image_files):
                    if i % 100 == 0:
                        print(f"  üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {i}/{len(self.image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_path = os.path.join(data_dir, img_file)
                    from PIL import Image
                    image = Image.open(image_path).convert('RGB')
                    self.images_ram[img_file] = image
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
                    prompt_file = img_file.replace('.png', '.txt')
                    prompt_path = os.path.join(data_dir, prompt_file)
                    with open(prompt_path, 'r', encoding='utf-8') as f:
                        prompt = f.read().strip()
                    self.prompts_ram[img_file] = prompt
                
                print(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.images_ram)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –û–ó–£")
                print(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.prompts_ram)} –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –û–ó–£")
        
        def __len__(self):
            return len(self.image_files)
        
        def __getitem__(self, idx):
            image_file = self.image_files[idx]
            
            # üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –û–ó–£ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if self.preload_to_ram and image_file in self.images_ram:
                # –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∏–∑ –û–ó–£
                image = self.images_ram[image_file]
                prompt = self.prompts_ram[image_file]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è CD —Ñ—É–Ω–∫—Ü–∏–∏
                import tempfile
                temp_image_path = f"/tmp/temp_image_{idx}.png"
                image.save(temp_image_path)
                
                return {
                    'image_path': temp_image_path,
                    'prompt': prompt
                }
            else:
                # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø —Å –¥–∏—Å–∫–∞
                image_path = os.path.join(self.data_dir, image_file)
                prompt_file = image_file.replace('.png', '.txt')
                prompt_path = os.path.join(self.data_dir, prompt_file)
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    prompt = f.read().strip()
                
                return {
                    'image_path': image_path,
                    'prompt': prompt
                }
    
    dataset = PixelDataset(data_dir, device)
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    num_epochs = 5  # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    max_iters = 4000  # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: 4000 –∏—Ç–µ—Ä–∞—Ü–∏–π
    batch_size = 4  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π batch size –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è OOM
    lr = 1e-4
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = torch.optim.SGD(student_model.parameters(), lr=lr, momentum=0.9)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32
    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º float32")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
    print("üîß –í–∫–ª—é—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏...")
    
    # TF32 –¥–ª—è Ampere GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
        torch.backends.cuda.matmul.allow_tf32 = True
        print("‚úÖ TF32 –≤–∫–ª—é—á–µ–Ω –¥–ª—è Ampere GPU")
    
    # Channels Last –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    try:
        student_model = student_model.to(memory_format=torch.channels_last)
        print("‚úÖ Channels Last –≤–∫–ª—é—á–µ–Ω")
    except Exception as e:
        print(f"‚ö†Ô∏è  Channels Last –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {e}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    all_losses = []
    start_time = time.time()
    
    # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
    log_memory_usage(0, 0, "üöÄ –°–¢–ê–†–¢:")
    
    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
    def log_detailed_memory(iteration, epoch, stage=""):
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_reserved = torch.cuda.memory_reserved() / 1024**3
            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_free = gpu_total - gpu_reserved
            
            print(f"üß† –ü–ê–ú–Ø–¢–¨ {stage}:")
            print(f"  –ò—Ç–µ—Ä–∞—Ü–∏—è: {iteration}, –≠–ø–æ—Ö–∞: {epoch}")
            print(f"  GPU Allocated: {gpu_allocated:.2f}GB")
            print(f"  GPU Reserved: {gpu_reserved:.2f}GB") 
            print(f"  GPU Free: {gpu_free:.2f}GB")
            print(f"  GPU Total: {gpu_total:.2f}GB")
            print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {(gpu_allocated/gpu_total)*100:.1f}%")
            print("-" * 50)
    
    print(f"\nüöÄ –ü–û–õ–ù–û–ï CD –û–ë–£–ß–ï–ù–ò–ï –° BATCH_SIZE = 6!")
    print(f"üìä –≠–ø–æ—Ö: {num_epochs}, –ò—Ç–µ—Ä–∞—Ü–∏–π: {max_iters}")
    print(f"üìä Batch Size: {batch_size} (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ!)")
    print(f"üìä VAE –Ω–∞ GPU: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è/–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è")
    print(f"üìä Gradient Checkpointing: –û–¢–ö–õ–Æ–ß–ï–ù (—É—Å–∫–æ—Ä–µ–Ω–∏–µ!)")
    print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {num_epochs * max_iters}")
    print(f"‚è±Ô∏è –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{num_epochs * max_iters * 0.4 / 60:.1f} –º–∏–Ω—É—Ç")
    print("=" * 70)
    
    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –Ω–∞—á–∞–ª–µ
    log_detailed_memory(0, 0, "–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    
    for epoch in range(num_epochs):
        print(f"\nüîÑ –≠–ü–û–•–ê {epoch + 1}/{num_epochs}")
        print("=" * 50)
        
        epoch_losses = []
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        pbar = tqdm(range(max_iters), desc=f"–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}")
        
        for iteration in pbar:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (batch size 2)
                batch_data = []
                
                for b in range(batch_size):
                    sample_idx = (iteration * batch_size + b) % len(dataset)
                    sample = dataset[sample_idx]
                    batch_data.append(sample)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü –∏–∑ –±–∞—Ç—á–∞ –¥–ª—è CD
                sample = batch_data[0]
                image_path = sample['image_path']
                prompt = sample['prompt']
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º CD —à–∞–≥
                loss = consistency_distillation_step(
                    image_path, prompt, teacher_model, student_model, 
                    text_encoder, tokenizer, vae, device
                )
                
                # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
                if iteration % 10 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
                    print(f"üìä –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}, Loss: {loss.item():.6f}")
                
                if loss is None:
                    continue
                
                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                optimizer.zero_grad()
                loss.backward(retain_graph=True)
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=0.5)
                
                optimizer.step()
                
                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                epoch_losses.append(loss.item())
                all_losses.append(loss.item())
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
                pbar.set_postfix({
                    'Loss': f"{loss.item():.6f}",
                    'Avg': f"{np.mean(epoch_losses):.6f}",
                    'Batch': f"{batch_size}"
                })
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –∏—Ç–µ—Ä–∞—Ü–∏–π
                if iteration % 10 == 0:
                    log_memory_usage(iteration, epoch + 1, "üß† –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò:")
                    log_detailed_memory(iteration, epoch + 1, "–î–ï–¢–ê–õ–¨–ù–´–ô –ú–û–ù–ò–¢–û–†–ò–ù–ì")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∫–∞–∂–¥—ã–µ 500 –∏—Ç–µ—Ä–∞—Ü–∏–π
                if iteration % 500 == 0 and iteration > 0:
                    try:
                        checkpoint = {
                            'model_state_dict': student_model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'epoch': epoch + 1,
                            'iteration': iteration,
                            'loss': loss.item(),
                            'batch_size': batch_size
                        }
                        torch.save(checkpoint, f'checkpoint_iter_{iteration}.pt')
                        print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: checkpoint_iter_{iteration}.pt")
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {e}")
                continue
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–æ—Ö–∏
        if len(epoch_losses) > 0:
            avg_loss = np.mean(epoch_losses)
            print(f"üìä –≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π loss: {avg_loss:.6f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —ç–ø–æ—Ö–∞–º
            try:
                torch.save(student_model.state_dict(), f'student_epoch_{epoch+1}.pt')
                print(f"üíæ –ú–æ–¥–µ–ª—å —ç–ø–æ—Ö–∏ {epoch+1} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: student_epoch_{epoch+1}.pt")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —ç–ø–æ—Ö–∏: {e}")
        else:
            print(f"üìä –≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π.")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å (–ü–†–û–í–ï–†–Ø–ï–ú —á—Ç–æ —ç—Ç–æ Student!)
    try:
        print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Student: dim={student_model.dim if hasattr(student_model, 'dim') else 'unknown'}")
        student_model_cpu = student_model.cpu()
        torch.save(student_model_cpu.state_dict(), 'student_test_cd_fixed_text_encoder.pt')
        student_model.to(device)
        print(f"üíæ –¢–µ—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: student_test_cd_fixed_text_encoder.pt")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å: {e}")
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    try:
        plt.figure(figsize=(12, 6))
        plt.plot(all_losses)
        plt.title('–¢–µ—Å—Ç–æ–≤–æ–µ CD –æ–±—É—á–µ–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú Text Encoder - –ü–æ—Ç–µ—Ä–∏')
        plt.xlabel('–ò—Ç–µ—Ä–∞—Ü–∏—è')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.savefig('test_cd_fixed_text_encoder_losses.png')
        print("üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: test_cd_fixed_text_encoder_losses.png")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫: {e}")
    
    total_time = time.time() - start_time
    print(f"\nüéâ –ü–û–õ–ù–û–ï CD –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    if len(all_losses) > 0:
        print(f"üìâ –ù–∞—á–∞–ª—å–Ω—ã–π loss: {all_losses[0]:.6f}")
        print(f"üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {all_losses[-1]:.6f}")
        if all_losses[0] > 0:
            improvement = ((all_losses[0] - all_losses[-1]) / all_losses[0] * 100)
            print(f"üìä –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.1f}%")
        else:
            print("üìä –£–ª—É—á—à–µ–Ω–∏–µ: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å (–Ω–∞—á–∞–ª—å–Ω—ã–π loss = 0)")
    else:
        print("‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è")
    print(f"üíæ –¢–µ—Å—Ç–æ–≤—ã–µ –≤–µ—Å–∞: student_test_cd_fixed_text_encoder.pt")
    print(f"üìä –¢–µ—Å—Ç–æ–≤—ã–π –≥—Ä–∞—Ñ–∏–∫: test_cd_fixed_text_encoder_losses.png")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    print(f"\nüé® –¢–ï–°–¢–ò–†–£–ï–ú –ì–ï–ù–ï–†–ê–¶–ò–Æ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
    print("=" * 50)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π VAE –¥–ª—è SDXL
        from diffusers import AutoencoderKL
        vae = AutoencoderKL.from_pretrained("stabilityai/sdxl-vae", torch_dtype=torch.float32)
        vae.to(device)
        vae.eval()
        print("‚úÖ VAE –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–¢–û–¢ –ñ–ï, —á—Ç–æ —É Teacher)")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
        test_prompts = [
            "A beautiful sunset over mountains",
            "A cozy cabin in a snowy forest",
            "A majestic dragon flying over a medieval castle"
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        os.makedirs("test_fixed_text_encoder_outputs", exist_ok=True)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        student_model.eval()
        with torch.no_grad():
            for i, prompt in enumerate(test_prompts):
                print(f"\nüìù –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º: '{prompt}'")
                
                # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                tokenized = tokenizer.tokenize(prompt)
                text_embeddings = text_encoder.encode(tokenized['input_ids'].to(device))[0].to(device, dtype=torch.float32)
                print(f"üìä –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {text_embeddings.shape}")
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã
                latents = torch.randn(1, 4, 64, 64, device=device, dtype=torch.float32)
                print(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ª–∞—Ç–µ–Ω—Ç—ã: {latents.shape}")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º (4 —à–∞–≥–∞) —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
                num_steps = 4
                print(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_steps} —à–∞–≥–æ–≤...")
                
                for step in tqdm(range(num_steps), desc=f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i+1}/3"):
                    t = torch.ones(1, device=device, dtype=torch.float32) * (1.0 - step / (num_steps - 1))
                    output = student_model(latents, t, text_embeddings)
                    latents = output['sample'] if isinstance(output, dict) else output
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (VAE –Ω–∞ GPU)
                latents_fp32 = latents.to(torch.float32)
                with torch.no_grad():
                    # VAE —É–∂–µ –Ω–∞ GPU
                    decoded_output = vae.decode(latents_fp32)
                decoded_image = decoded_output.sample if hasattr(decoded_output, 'sample') else decoded_output
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)
                image_tensor = decoded_image[0].cpu()
                image_array = (image_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)
                image_array = np.clip(image_array, 0, 255)
                
                from PIL import Image
                image = Image.fromarray(image_array)
                filename = f"test_fixed_text_encoder_outputs/test_generated_{i+1}.png"
                image.save(filename)
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
        
        print(f"\nüé® –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: test_fixed_text_encoder_outputs/")
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –∫–æ–Ω—Ü–µ
        log_detailed_memory(max_iters, num_epochs, "–ö–û–ù–ï–¶ –û–ë–£–ß–ï–ù–ò–Ø")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    train_cd_fixed_text_encoder()
