#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ü–†–ê–í–ò–õ–¨–ù–û–ô Consistency Distillation –Ω–∞ 20 –∏—Ç–µ—Ä–∞—Ü–∏–π
–ü—Ä–∏–Ω—Ü–∏–ø: Student –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å x_0 –∏–∑ –ª—é–±–æ–≥–æ X_t
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
import time
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ micro_diffusion
sys.path.append('/home/ubuntu/train/train/micro_diffusion')

def load_models(device="cuda"):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...")
    
    # Student –º–æ–¥–µ–ª—å - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö)
    from micro_diffusion.models.dit import DiT
    student_model = DiT(
        input_size=64,
        patch_size=2,
        in_channels=4,
        dim=1152,  # –í–µ—Ä–Ω—É–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        depth=28,  # –í–µ—Ä–Ω—É–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É
        head_dim=64,
        multiple_of=256,
        caption_channels=1024,
        pos_interp_scale=1.0,
        norm_eps=1e-6,
        depth_init=True,
        qkv_multipliers=[1.0],
        ffn_multipliers=[4.0],
        use_patch_mixer=True,
        patch_mixer_depth=4,  # –í–µ—Ä–Ω—É–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        patch_mixer_dim=512,  # –í–µ—Ä–Ω—É–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        patch_mixer_qkv_ratio=1.0,
        patch_mixer_mlp_ratio=1.0,
        use_bias=True,
        num_experts=8,  # –í–µ—Ä–Ω—É–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        expert_capacity=1,
        experts_every_n=2
    )
    student_model.to(device)
    student_model.train()
    
    # VAE
    vae_path = "/home/ubuntu/train/train/vae_model.pt"
    if os.path.exists(vae_path):
        vae_checkpoint = torch.load(vae_path, map_location=device)
        from micro_diffusion.models.autoencoder import Autoencoder
        vae = Autoencoder()
        vae.load_state_dict(vae_checkpoint['model_state_dict'])
        vae.to(device)
        vae.eval()
        print("‚úÖ VAE –∑–∞–≥—Ä—É–∂–µ–Ω")
    else:
        print("‚ùå VAE –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        vae = None
    
    # Text Encoder
    text_encoder_path = "/home/ubuntu/train/train/text_encoder.pt"
    if os.path.exists(text_encoder_path):
        text_encoder_checkpoint = torch.load(text_encoder_path, map_location=device)
        from micro_diffusion.models.text_encoder import TextEncoder
        text_encoder = TextEncoder()
        text_encoder.load_state_dict(text_encoder_checkpoint['model_state_dict'])
        text_encoder.to(device)
        text_encoder.eval()
        print("‚úÖ Text Encoder –∑–∞–≥—Ä—É–∂–µ–Ω")
    else:
        print("‚ùå Text Encoder –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        text_encoder = None
    
    return student_model, vae, text_encoder

def consistency_distillation_step(latents, text_embeddings, student_model, device="cuda"):
    """
    –ü–†–ê–í–ò–õ–¨–ù–ê–Ø Consistency Distillation:
    Student –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å x_0 –∏–∑ –ª—é–±–æ–≥–æ X_t
    """
    batch_size = latents.shape[0]
    
    # 1. x_0 - —á–∏—Å—Ç—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã (—Ü–µ–ª—å!)
    x_0 = latents
    
    # 2. –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π timestep
    t = torch.rand(batch_size, device=device)
    
    # 3. –ó–∞—à—É–º–∏–≤–∞–µ–º x_0 –¥–æ x_t
    noise = torch.randn_like(latents)
    x_t = x_0 + noise * t.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
    
    # 4. Student –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0 –∏–∑ x_t
    # DiT –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (x, t, y) –≥–¥–µ y - text embeddings
    student_output = student_model(x_t, t, text_embeddings)
    student_pred = student_output['sample'] if isinstance(student_output, dict) else student_output
    
    # 5. –û–°–ù–û–í–ù–û–ô LOSS - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ x_0!
    loss_prediction = F.mse_loss(student_pred, x_0)
    
    # 6. CONSISTENCY CONSTRAINT - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ timestep –¥–ª—è consistency (–º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)
    loss_consistency = torch.tensor(0.0, device=device)  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    
    # 7. BOUNDARY CONDITION - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø  
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ prediction –¥–ª—è boundary (–º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)
    loss_boundary = torch.tensor(0.0, device=device)  # –£–ø—Ä–æ—â–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    
    # 8. –û–±—â–∏–π loss
    total_loss = loss_prediction + 0.1 * loss_consistency + 0.1 * loss_boundary
    
    return {
        'total_loss': total_loss,
        'prediction_loss': loss_prediction,
        'consistency_loss': loss_consistency,
        'boundary_loss': loss_boundary
    }

def test_consistency_distillation():
    """–¢–µ—Å—Ç Consistency Distillation –Ω–∞ 20 –∏—Ç–µ—Ä–∞—Ü–∏–π"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    student_model, vae, text_encoder = load_models(device)
    if student_model is None:
        return
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º SGD –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö
    optimizer = torch.optim.SGD(student_model.parameters(), lr=1e-4, momentum=0.9)
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 1
    latent_size = 64
    channels = 4
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã
    test_latents = torch.randn(batch_size, channels, latent_size, latent_size, device=device)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
    test_prompt = "A beautiful sunset over mountains"
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if text_encoder is not None:
        with torch.no_grad():
            tokenized = text_encoder.tokenizer.tokenize([test_prompt])
            input_ids = tokenized['input_ids'].to(device)
            text_embeddings = text_encoder.encode(input_ids)[0]
            if text_embeddings.dim() == 4:
                text_embeddings = text_embeddings.squeeze(1)
    else:
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è - –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è DiT
        # DiT –æ–∂–∏–¥–∞–µ—Ç caption_channels=1024, –Ω–æ –º—ã –ø–µ—Ä–µ–¥–∞–µ–º 1152
        # –ù—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å caption_channels –≤ DiT –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        text_embeddings = torch.randn(batch_size, 77, 1024, device=device)  # –ò–∑–º–µ–Ω–∏–ª–∏ –Ω–∞ 1024
    
    print(f"üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º Consistency Distillation...")
    print(f"üìä –†–∞–∑–º–µ—Ä—ã: latents={test_latents.shape}, text_embeddings={text_embeddings.shape}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º 20 –∏—Ç–µ—Ä–∞—Ü–∏–π
    losses = []
    
    print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 20 –∏—Ç–µ—Ä–∞—Ü–∏–π...")
    
    for iteration in tqdm(range(20), desc="–û–±—É—á–µ–Ω–∏–µ"):
        optimizer.zero_grad()
        
        # Consistency Distillation step
        loss_dict = consistency_distillation_step(
            test_latents, text_embeddings, student_model, device
        )
        
        # Backward pass
        loss_dict['total_loss'].backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        losses.append({
            'iteration': iteration,
            'total_loss': loss_dict['total_loss'].item(),
            'prediction_loss': loss_dict['prediction_loss'].item(),
            'consistency_loss': loss_dict['consistency_loss'].item(),
            'boundary_loss': loss_dict['boundary_loss'].item()
        })
        
        if iteration % 5 == 0:
            print(f"  –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}:")
            print(f"    üìâ Total Loss: {loss_dict['total_loss'].item():.6f}")
            print(f"    üéØ Prediction Loss: {loss_dict['prediction_loss'].item():.6f}")
            print(f"    üîÑ Consistency Loss: {loss_dict['consistency_loss'].item():.6f}")
            print(f"    üéØ Boundary Loss: {loss_dict['boundary_loss'].item():.6f}")
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 5 –∏—Ç–µ—Ä–∞—Ü–∏–π
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print("=" * 50)
    
    final_loss = losses[-1]['total_loss']
    initial_loss = losses[0]['total_loss']
    improvement = (initial_loss - final_loss) / initial_loss * 100
    
    print(f"üìâ –ù–∞—á–∞–ª—å–Ω—ã–π loss: {initial_loss:.6f}")
    print(f"üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {final_loss:.6f}")
    print(f"üìä –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.2f}%")
    
    if improvement > 5:
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è! Loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è")
    elif improvement > 0:
        print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ")
    else:
        print("‚ùå –ú–æ–¥–µ–ª—å –ù–ï –æ–±—É—á–∞–µ—Ç—Å—è!")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    print(f"\nüé® –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
    
    with torch.no_grad():
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑ —à—É–º–∞
        noise = torch.randn_like(test_latents)
        t_start = torch.ones(batch_size, device=device) * 0.9  # –ù–∞—á–∏–Ω–∞–µ–º —Å –≤—ã—Å–æ–∫–æ–≥–æ —à—É–º–∞
        
        # –û–¥–∏–Ω —à–∞–≥ Student –º–æ–¥–µ–ª–∏
        student_output = student_model(noise, t_start, text_embeddings)
        student_pred = student_output['sample'] if isinstance(student_output, dict) else student_output
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
        print(f"  Mean: {student_pred.mean().item():.6f}")
        print(f"  Std: {student_pred.std().item():.6f}")
        print(f"  Min: {student_pred.min().item():.6f}")
        print(f"  Max: {student_pred.max().item():.6f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å x_0
        target = test_latents
        mse_to_target = F.mse_loss(student_pred, target).item()
        print(f"üéØ MSE –∫ —Ü–µ–ª–µ–≤–æ–º—É x_0: {mse_to_target:.6f}")
        
        if mse_to_target < 1.0:
            print("‚úÖ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0!")
        else:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç x_0")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    try:
        student_model_cpu = student_model.cpu()
        torch.save(student_model_cpu.state_dict(), 'student_consistency_20iters.pt')
        student_model.to(device)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞ GPU
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: student_consistency_20iters.pt")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        print(f"üíæ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot([l['total_loss'] for l in losses])
    plt.title('Total Loss')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    
    plt.subplot(2, 2, 2)
    plt.plot([l['prediction_loss'] for l in losses])
    plt.title('Prediction Loss (Student -> x_0)')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    
    plt.subplot(2, 2, 3)
    plt.plot([l['consistency_loss'] for l in losses])
    plt.title('Consistency Loss')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    
    plt.subplot(2, 2, 4)
    plt.plot([l['boundary_loss'] for l in losses])
    plt.title('Boundary Loss (t=0 -> x_0)')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    
    plt.tight_layout()
    plt.savefig('consistency_distillation_20iters.png', dpi=150, bbox_inches='tight')
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: consistency_distillation_20iters.png")
    
    return losses

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –¢–ï–°–¢ –ü–†–ê–í–ò–õ–¨–ù–û–ô CONSISTENCY DISTILLATION")
    print("=" * 60)
    print("üéØ –ü—Ä–∏–Ω—Ü–∏–ø: Student –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å x_0 –∏–∑ –ª—é–±–æ–≥–æ X_t")
    print("üéØ Boundary Condition: f(X_0, t=0) = x_0")
    print("üéØ Consistency: f(X_t1, t1) = f(X_t2, t2) = x_0")
    print("üîÑ –ò—Ç–µ—Ä–∞—Ü–∏–π: 20")
    print("=" * 60)
    
    try:
        start_time = time.time()
        losses = test_consistency_distillation()
        end_time = time.time()
        
        print(f"\nüéâ –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù!")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä–∞—Ñ–∏–∫: consistency_distillation_20iters.png")
        print(f"üíæ –ú–æ–¥–µ–ª—å: student_consistency_20iters.pt")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
