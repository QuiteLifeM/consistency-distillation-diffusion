"""
–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
"""
import torch
import os

def test_cuda():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA"""
    print("="*60)
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
    print(f"   –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   –°–≤–æ–±–æ–¥–Ω–æ: {torch.cuda.memory_reserved(0) / 1024**3:.1f} GB")
    return True

def test_imports():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤"""
    print("\n" + "="*60)
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤")
    print("="*60)
    
    try:
        print("1. –ò–º–ø–æ—Ä—Ç torch...")
        import torch
        print(f"   ‚úÖ PyTorch {torch.__version__}")
        
        print("2. –ò–º–ø–æ—Ä—Ç micro_diffusion...")
        from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion
        print("   ‚úÖ micro_diffusion –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        
        return True
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        return False

def test_model_creation():
    """–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    print("\n" + "="*60)
    print("–¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏")
    print("="*60)
    
    try:
        from micro_diffusion.micro_diffusion.models.model import create_latent_diffusion
        
        print("1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CPU...")
        model = create_latent_diffusion(
            latent_res=32,  # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä
            in_channels=4,
            pos_interp_scale=1.0,  # –ú–µ–Ω—å—à–∏–π scale
            precomputed_latents=False,
            dtype="float32"
        )
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –Ω–∞ CPU")
        
        print("2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏...")
        print(f"   - DiT: {type(model.dit)}")
        print(f"   - VAE: {type(model.vae)}")
        print(f"   - Text Encoder: {type(model.text_encoder)}")
        print(f"   - Tokenizer: {type(model.tokenizer)}")
        
        print("3. –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ GPU...")
        try:
            model = model.to("cuda")
            print("   ‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ GPU")
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –Ω–∞ GPU: {e}")
            print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å CPU")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        return False

def test_teacher_weights():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ —É—á–∏—Ç–µ–ª—è"""
    print("\n" + "="*60)
    print("–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ —É—á–∏—Ç–µ–ª—è")
    print("="*60)
    
    teacher_path = "./micro_diffusion/trained_models/teacher.pt"
    
    if not os.path.exists(teacher_path):
        print(f"‚ùå –§–∞–π–ª –≤–µ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {teacher_path}")
        return False
    
    try:
        print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤...")
        weights = torch.load(teacher_path, map_location="cpu")
        print(f"   ‚úÖ –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, —Ä–∞–∑–º–µ—Ä: {os.path.getsize(teacher_path) / 1024**2:.1f} MB")
        
        print("2. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–µ—Å–æ–≤...")
        print(f"   –ö–ª—é—á–µ–π –≤ –≤–µ—Å–∞—Ö: {len(weights)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π
        keys = list(weights.keys())[:5]
        for key in keys:
            shape = weights[key].shape if hasattr(weights[key], 'shape') else 'N/A'
            print(f"   - {key}: {shape}")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤: {e}")
        return False

if __name__ == "__main__":
    print("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏")
    print("="*60)
    
    # –¢–µ—Å—Ç 1: CUDA
    cuda_ok = test_cuda()
    
    # –¢–µ—Å—Ç 2: –ò–º–ø–æ—Ä—Ç—ã
    imports_ok = test_imports()
    
    if not imports_ok:
        print("\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏")
        exit(1)
    
    # –¢–µ—Å—Ç 3: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_ok = test_model_creation()
    
    # –¢–µ—Å—Ç 4: –í–µ—Å–∞ —É—á–∏—Ç–µ–ª—è
    weights_ok = test_teacher_weights()
    
    print("\n" + "="*60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò")
    print("="*60)
    print(f"CUDA: {'‚úÖ' if cuda_ok else '‚ùå'}")
    print(f"–ò–º–ø–æ—Ä—Ç—ã: {'‚úÖ' if imports_ok else '‚ùå'}")
    print(f"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {'‚úÖ' if model_ok else '‚ùå'}")
    print(f"–í–µ—Å–∞ —É—á–∏—Ç–µ–ª—è: {'‚úÖ' if weights_ok else '‚ùå'}")
    
    if cuda_ok and imports_ok and model_ok and weights_ok:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–®–õ–ò! –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Ä–∞–±–æ—Ç–∞—Ç—å.")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å: python train.py")
    else:
        print("\n‚ö†Ô∏è –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        if not cuda_ok:
            print("- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É CUDA –∏ PyTorch")
        if not model_ok:
            print("- –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å latent_res –¥–æ 32")
            print("- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π")
        if not weights_ok:
            print("- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É teacher.pt")
    
    print("="*60)



